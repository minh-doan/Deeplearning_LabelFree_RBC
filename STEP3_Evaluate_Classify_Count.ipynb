{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os.path\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy\n",
    "import pandas\n",
    "import pickle\n",
    "\n",
    "import tensorflow\n",
    "import keras\n",
    "import deepometry.model\n",
    "import sklearn.metrics\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "#from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect_pathnames(directories, labels, n_samples):\n",
    "    \"\"\"\n",
    "    :param directories: List of directories to select samples from. Assumes subdirectories of each directory\n",
    "                        correspond to class labels. Contents of subdirectories are NPY files containing data\n",
    "                        of that label.\n",
    "    :param n_samples: How many cells (each class) to be evaluated & visualized\n",
    "    :return: List of pathnames.\n",
    "    \"\"\"\n",
    "    pathnames = []\n",
    "\n",
    "    for directory in directories:\n",
    "        subdirectories = sorted(glob.glob(os.path.join(directory, \"*\")))\n",
    "        \n",
    "        # transform the files of the same label into directory\n",
    "        filelist = [glob.glob(\"{}/*{}*\".format(subdirectory,label)) for label in labels for subdirectory in subdirectories ]\n",
    "        \n",
    "        subdirectory_pathnames = []\n",
    "        for i in range(len(labels)):\n",
    "            a = filelist[i*len(subdirectories):(i+1)*len(subdirectories)]\n",
    "            subdirectory_pathnames.append( list(itertools.chain.from_iterable(a)) )        \n",
    "\n",
    "        if n_samples == 'max' :\n",
    "            nsamples = max([len(pathnames) for pathnames in subdirectory_pathnames])\n",
    "        else:\n",
    "            if n_samples == 'min':\n",
    "                nsamples = min([len(pathnames) for pathnames in subdirectory_pathnames])\n",
    "            else:\n",
    "                nsamples = n_samples\n",
    "\n",
    "        pathnames += [list(numpy.random.permutation(pathnames)[:nsamples]) for pathnames in subdirectory_pathnames]\n",
    "\n",
    "    pathnames = sum(pathnames, [])\n",
    "\n",
    "    return pathnames\n",
    "\n",
    "\n",
    "def load(pathnames, labels, dates):\n",
    "    \"\"\"\n",
    "    Load training and target data.\n",
    "    \n",
    "    Assumes data is stored in a directory corresponding to some class label.\n",
    "\n",
    "    :param pathnames: List of image pathnames.\n",
    "    :param labels: List of class labels.\n",
    "    :return: Tuple (training, target) data, as NumPy arrays.\n",
    "    \"\"\"\n",
    "    #--- if you want to ignore some class of morphology ---#\n",
    "    #pathnames = [x for x in pathnames if ((\"renated\" in x) and (\"oid\" not in x))]\n",
    "    #pathnames = [x for x in pathnames if \"Smooth Sphere\" not in x]\n",
    "    \n",
    "    x = numpy.empty((len(pathnames),) + _shape(pathnames[0]), dtype=numpy.uint8)\n",
    "\n",
    "    y = numpy.empty((len(pathnames),), dtype=numpy.uint8)\n",
    "    \n",
    "    z = numpy.empty((len(pathnames),), dtype=numpy.uint8)\n",
    "    \n",
    "    m = numpy.empty((len(pathnames),), dtype=numpy.uint8)\n",
    "\n",
    "    label_to_index = {label: index for index, label in enumerate(sorted(labels))}\n",
    "    \n",
    "    day_to_index = {day: index for index, day in enumerate(day_of_exp)}\n",
    "    \n",
    "    label_to_m_index = {\"Smooth Disc\": 6, \"Smooth Sphere\": 1, \"Crenated Discoid\" : 4, \"Crenated Disc_\" : 5, \"Crenated Spheroid\": 3, \"Crenated Spheres\": 2}\n",
    "\n",
    "    for index, pathname in enumerate(pathnames):\n",
    "        if os.path.isfile(pathname) == True:\n",
    "\n",
    "            day = re.search('parsed_data.Bag...(.*)', os.path.dirname(pathname)).group(1)\n",
    "            \n",
    "            label = re.search('- (.*)_Total', os.path.basename(pathname) ).group(1)\n",
    "            if label == 'Crenated Disc':\n",
    "                label = str(label + \"_\")\n",
    "\n",
    "            x[index] = numpy.load(pathname)\n",
    "\n",
    "            y[index] = label_to_index[label]\n",
    "            \n",
    "            z[index] = day_to_index[day]\n",
    "            \n",
    "            m[index] = label_to_m_index[label]\n",
    " \n",
    "    return x, y, z, m\n",
    "\n",
    "def _shape(pathname):\n",
    "    \"\"\"\n",
    "    Infer the shape of the sample data from a single sample.\n",
    "    \n",
    "    :param pathname: Path to a sample.\n",
    "    :return: Sample dimensions.\n",
    "    \"\"\"\n",
    "    return numpy.load(pathname).shape\n",
    "\n",
    "def get_immediate_subdirectories(a_dir):\n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if os.path.isdir(os.path.join(a_dir, name))]\n",
    "\n",
    "def save_metadata_label(label,labels,day,days,file):\n",
    "    with open(file, 'w') as f:\n",
    "        f.write('Day\\tLabel\\n')\n",
    "        for i in range(label.shape[0]):              \n",
    "            f.write('{}\\t{}\\n'.format( days[day[i]] , list(sorted(labels))[label[i]]))    \n",
    "\n",
    "def save_metadata_numericday(day,file):\n",
    "    with open(file, 'w') as f:\n",
    "        for i in range(day.shape[0]):\n",
    "            f.write('{}\\n'.format( day[i] ))      \n",
    "            \n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_bags = ['A', 'B', 'C', 'D', 'E','F', 'H']\n",
    "test_bags = ['J']\n",
    "test_bags = [str(bag + str(i+1)) for bag in test_bags for i in range(3)]\n",
    "\n",
    "labels = [\"Smooth Disc\", \"Crenated Disc_\", \"Crenated Discoid\", \"Crenated Spheroid\", \"Crenated Spheres\",\"Smooth Sphere\"]\n",
    "day_of_exp = [str('D' + str(i)) for i in list(range(1,46))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build session running on GPU 1\n",
    "configuration = tensorflow.ConfigProto()\n",
    "configuration.gpu_options.allow_growth = True\n",
    "configuration.gpu_options.visible_device_list = \"1\"\n",
    "session = tensorflow.Session(config = configuration)\n",
    "\n",
    "# apply session\n",
    "keras.backend.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = deepometry.model.Model(shape=(48,48,2), units=6)\n",
    "\n",
    "model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.model.load_weights('/models/deepometry_BFDF_1xMin_ResNet50_fast_6bags_88/deepometry/data/checkpoint.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised classification and count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = '/models/BFDF_1xMin_ResNet50_fast_6bags_88/count_and_predict/'\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "     os.makedirs(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs_count_per_bag = []\n",
    "dl_count_per_bag = []\n",
    "for test_bag in test_bags:\n",
    "    print(test_bag)\n",
    "\n",
    "    directories = [str('/parsed_data/Bag'+test_bag)]\n",
    "\n",
    "    for directory in directories:\n",
    "        subdirectories = sorted(glob.glob(os.path.join(directory, \"*\")), key = lambda l: int(os.path.basename(l)[1:]))\n",
    "        \n",
    "        abs_count_per_day = []\n",
    "        dl_count_per_day = []\n",
    "        for subdirectory in subdirectories:\n",
    "            #print(os.path.basename(subdirectory))\n",
    "            \n",
    "            abs_count_per_label = []\n",
    "            dl_count_per_label = []\n",
    "            for label in labels:\n",
    "                #print(label, \" :\", len(glob.glob(\"{}/*{}*\".format(subdirectory,label))))\n",
    "                \n",
    "                pathnames = glob.glob(\"{}/*{}*\".format(subdirectory,label))\n",
    "                abs_count_per_label.append(len(pathnames))\n",
    "                \n",
    "                if len(pathnames) > 0 :\n",
    "\n",
    "                    xx, y, z, m = load(pathnames, labels, day_of_exp)\n",
    "                    x = xx[:,:,:,0:3:2]                    \n",
    "\n",
    "                    predicted = model.predict(\n",
    "                        batch_size=50,\n",
    "                        x=x\n",
    "                    )\n",
    "\n",
    "                    predicted = numpy.argmax(predicted, -1)\n",
    "\n",
    "                    expected = y\n",
    "\n",
    "                    confusion = sklearn.metrics.confusion_matrix(expected, predicted)\n",
    "                    \n",
    "                    dl_count_per_label.append(numpy.max(confusion))\n",
    "\n",
    "\n",
    "                    del(xx,x,y,z,m)\n",
    "                \n",
    "            abs_count_per_day.append(abs_count_per_label)\n",
    "            dl_count_per_day.append(dl_count_per_label)\n",
    "            \n",
    "        \n",
    "        abs_count_per_bag.append(abs_count_per_day)\n",
    "        dl_count_per_bag.append(dl_count_per_day)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filehandler = open(\"/models/BFDF_1xMin_ResNet50_fast_6bags_88/count_and_predict/abs_count_per_bag_TEST.sav\", \"wb\")\n",
    "# pickle.dump(abs_count_per_bag,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# filehandler = open(\"/models/BFDF_1xMin_ResNet50_fast_6bags_88/count_and_predict/dl_count_per_bag_TEST.sav\", \"wb\")\n",
    "# pickle.dump(dl_count_per_bag,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for test_bag in test_bags:\n",
    "    print(test_bag)\n",
    "\n",
    "    directories = [str('/media/paul/5c2fed7b-3e9d-4a9c-8c8f-d03b917ab93d/home/paul/Minh/RBC/parsed_BF_DF/Bag'+test_bag)]\n",
    "\n",
    "    pathnames = collect_pathnames(directories, labels, n_samples = 'max')\n",
    "\n",
    "    xx, y, z, m = load(pathnames, labels, day_of_exp)\n",
    "    x = xx[:,:,:,0:3:2]    \n",
    "\n",
    "    print(\"x: \", x.shape)\n",
    "    print(\"y: \", y.shape)\n",
    "    print(Counter(y))\n",
    "    print(\"z: \",Counter(z))\n",
    "    print(\"m: \",Counter(m)) \n",
    "    \n",
    "    #model.evaluate(x, y, batch_size = 64, verbose=1)\n",
    "        \n",
    "    predicted = model.predict(\n",
    "        batch_size=50,\n",
    "        x=x\n",
    "    )\n",
    "\n",
    "    predicted = numpy.argmax(predicted, -1)\n",
    "\n",
    "    expected = y\n",
    "    \n",
    "    confusion = sklearn.metrics.confusion_matrix(expected, predicted)\n",
    "    \n",
    "    numpy.save(os.path.join(output_directory, str('confusion_matrix_absolute_TEST_'+test_bag+'.npy')), confusion)\n",
    "    \n",
    "#     # Normalized CM\n",
    "\n",
    "#     confusion = confusion.astype('float') / confusion.sum(axis=1)[:, numpy.newaxis]\n",
    "\n",
    "#     confusion = pandas.DataFrame(confusion)\n",
    "\n",
    "#     matplotlib.pyplot.figure(figsize=(12, 8))\n",
    "\n",
    "#     seaborn.heatmap(confusion, annot=True)\n",
    "\n",
    "#     matplotlib.pyplot.savefig( os.path.join(output_directory, str('confusion_matrix_percent_TEST_'+test_bag+'.png')) , dpi=300)\n",
    "    \n",
    "    del(xx,x,y,z,m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
